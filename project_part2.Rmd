---
title: "Application Part Big Data for Economists Seminar"
author: "Marc Richter and Jingyan Yang"
date: "25 4 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Set up work space

```{r}

rm(list = ls())

library(rstudioapi)
library(mice)
library(dplyr)
library(Hmisc)
library(randomForest)
library(doParallel)

setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

train_data <- read.csv("./training.csv")

X_test <- read.csv("./X_test.csv")


```

## Overview our data

Let's have a look at our data. We have no data set information/fact sheet so we will have to figure out the content ourselves...

```{r, echo=FALSE}
#head(train_data)

summary(train_data)

```

We have the following columns:

* ID: just the row ID
* address: addresses of the homes . sometimes with street name, street number, sometimes only postcode and town... pretty useless. we have the zipcode in an additional column anyway
* area: should be the total built area of the home.
* area_useable: the area of the home including land
* date: date of advertisement release
* date available
* home_type , ca. 190 categories
* municipality . probably won't use, have post code
* newly_built
* price : our dependent variable in this exercise
* rooms : number of rooms
* street: street name, won't use
* year: year of advertisment release
* year_built
* zipcode
* number of dummy variables:
  + balcony
  + basemnt
  + bath_tube
  + building_plot
  + cabletv
  + ceiling
  + cheminee
  + elevator
  + first_time
  + furnished
  + kids_friendly
  + laundry
  + minergie
  + municipality
  + new building
  + oldbuilding
  + oven
  + parking_indoor
  + parking_outside
  + playground
  + pool
  + quiet
  + raisedgroundfloor
  + sale
  + sunny
  + terrace
  + topstorage
  + veranda
  + wheelchair

```{r}
# code from https://www.r-bloggers.com/imputing-missing-data-with-r-mice-package/

pMiss <- function(x){sum(is.na(x))/length(x)*100}
apply(train_data,2, pMiss)
```

This looks... bad ..^^

Leaving out all rows with missing data is not an option, as it would leave us with pretty much no data (and for additional reasons like possible bias - the data is most likely not missing completely at random).

One major problem: our dummy variable all only contain ones and NA's. This way, we have no way of knowing which of the NA's are actually NA's and which are 0s.

First approach: Coding all NA's in dummy vars to 0. New meaning of predictor: not "does have ..." but "characteristic ... is mentioned".

```{r}

# i have done this manually because i didn't find a nice solution in R to select columns conditional on mean , min and max (would all be 1)
dummy_cols <- c("balcony", "basement", "bath_tube", "building_plot", "cabletv", "ceiling", "cheminee", "elevator", "first_time", "furnished", "kids_friendly", "laundry", "minergie", "new_building", "oldbuilding", "oven", "parking_indoor", "parking_outside", "playground", "pool", "quiet", "raised_groundfloor", "sale", "sunny", "terrace", "topstorage", "veranda", "wheelchair")

train_without_dummyNA <- train_data %>% mutate_at(.vars = dummy_cols, .funs = funs(ifelse(is.na(.), 0, .)))

apply(train_without_dummyNA,2, pMiss)

summary(train_without_dummyNA)

######### Hi Marc! before imputing NAs I add and delete some vars. #########
# For "new_building", it contains too many missing value, we can remove this variable;
# For "new_built" and "year_built", we can find some observations a bit confusing:(
# Most apartments built after 2011 are defined as "new_built" (label 1 for variable "new_built"), but some observations built in the same year are labelled diffently in "new_built". Also, the value of "new_building" and "new_built" are not equal to each other all the time.

## multiple core computation


# Counts the number of cores on your machine
K <- parallel::detectCores()

# This registers each core as a member of a
# computing cluster

cl <- makeCluster(K)

registerDoParallel(cl)

# delete some useless or redundant variables
train_imputed <- subset(train_without_dummyNA, select = -c(id, address, area_useable, date, date_available, municipality, street))
apply(train_imputation,2, pMiss)

# using "pmm" method from MICE package, as it needs quite a lot computation we set m=1 and maxit=1
  train_imputed_pmm <- complete(mice(train_imputed, m=1, maxit = 5, meth = "pmm", seed = 50), 1) 
write.csv(train_imputed_pmm, file = "./train_imputed_pmm.csv", row.names = FALSE)

# using "rf" method from MICE package
  train_imputed_rf <- complete(mice(train_imputed, m=1, maxit = 5, meth = "rf", seed = 50), 1) 
write.csv(train_imputed_rf, file = "./train_imputed_rf.csv", row.names = FALSE)
#apply(train_imputation_rf,2, pMiss)

# Once you finished, you deactivate the cluster
# with these two commands.
stopCluster(cl)
registerDoSEQ()

```

## Load imputed data froms stored CSV files

```{r}
train_imputed_pmm <- read.csv("./train_imputation_pmm.csv")
train_imputed_rf <- read.csv("./train_imputation_rf.csv")

# merge with price

train_imputed_pmm <- train_imputed_pmm %>% left_join()
```

Outlier detection and standardization.

```{r}


```


We will do further adjustments to our data. 

```{r}

# var format transformations

train_without_dummyNA$price <- as.integer(train_without_dummyNA$price) # price is a factor variable otherwise
train_without_dummyNA$zipcode <- as.factor(train_without_dummyNA$zipcode) # zipcode should be factor - could even get more sophisticated (e.g. cantone...) with this one if time allows - the tax rate substantially differs across municipatlities in switzerland - that might well affect the prices of aparments




train_lin_subset <- train_without_dummyNA %>% select(-c("address", "municipality", "street", "year", "date", "date_available", "zipcode"))
```

Trying out a linear regression with k fold cross validation.

```{r}

lin_fit <- cv.lm(train_imputed_pmm, form.lm = formula(price ~.))

```


```{r}
md.pattern(train_lin_subset)
```

Hello Jingyan !;))
