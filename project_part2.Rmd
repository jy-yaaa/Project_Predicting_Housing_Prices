---
title: "Application Part Big Data for Economists Seminar"
author: "Marc Richter and Jingyan Yang"
date: "25 4 2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Set up work space

```{r}

rm(list = ls())

library(rstudioapi)
library(mice)
library(tidyverse)
library(Hmisc)
library(randomForest)
library(doParallel)
library(caret)
library(glmnet)
library(MASS) # for stepAIC formula
library(leaps) # for regsubsets model selection
library(tree)

setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

train_data <- read.csv("./training.csv")

X_test <- read.csv("./X_test.csv")


```

## Overview our data

Let's have a look at our data. We have no data set information/fact sheet so we will have to figure out the content ourselves...

```{r, echo=FALSE}
#head(train_data)

summary(train_data)

```

We have the following columns:

* ID: just the row ID
* address: addresses of the homes . sometimes with street name, street number, sometimes only postcode and town... pretty useless. we have the zipcode in an additional column anyway
* area: should be the total built area of the home.
* area_useable: the area of the home including land
* date: date of advertisement release
* date available
* home_type , ca. 190 categories
* municipality . probably won't use, have post code
* newly_built
* price : our dependent variable in this exercise
* rooms : number of rooms
* street: street name, won't use
* year: year of advertisment release
* year_built
* zipcode
* number of dummy variables:
  + balcony
  + basemnt
  + bath_tube
  + building_plot
  + cabletv
  + ceiling
  + cheminee
  + elevator
  + first_time
  + furnished
  + kids_friendly
  + laundry
  + minergie
  + municipality
  + new building
  + oldbuilding
  + oven
  + parking_indoor
  + parking_outside
  + playground
  + pool
  + quiet
  + raisedgroundfloor
  + sale
  + sunny
  + terrace
  + topstorage
  + veranda
  + wheelchair

```{r}
# code from https://www.r-bloggers.com/imputing-missing-data-with-r-mice-package/

pMiss <- function(x){sum(is.na(x))/length(x)*100}
apply(train_data,2, pMiss)
```

This looks... bad ..^^

Leaving out all rows with missing data is not an option, as it would leave us with pretty much no data (and for additional reasons like possible bias - the data is most likely not missing completely at random).

One major problem: our dummy variable all only contain ones and NA's. This way, we have no way of knowing which of the NA's are actually NA's and which are 0s.

First approach: Coding all NA's in dummy vars to 0. New meaning of predictor: not "does have ..." but "characteristic ... is mentioned".

```{r}

dummy_cols <- c("balcony", "basement", "bath_tube", "building_plot", "cabletv", "ceiling", "cheminee", "elevator", "first_time", "furnished", "kids_friendly", "laundry", "minergie", "new_building", "oldbuilding", "oven", "parking_indoor", "parking_outside", "playground", "pool", "quiet", "raised_groundfloor", "sale", "sunny", "terrace", "topstorage", "veranda", "wheelchair")

train_without_dummyNA <- train_data %>% mutate_at(.vars = dummy_cols, .funs = funs(ifelse(is.na(.), 0, .)))

apply(train_without_dummyNA,2, pMiss)
```

Let us check for some specific numeric variables, if there are some significant outliers in the dataset.

```{r}
plot(train_without_dummyNA$area) # not real outliers,but when check manually some observations with area=1, which does not make sense...
plot(train_without_dummyNA$floors) # not real outliers
plot(train_without_dummyNA$price) # not real outliers
plot(train_without_dummyNA$rooms) # not real outliers
plot(train_without_dummyNA$year_built) # we could delete the obs. from 1200

train_without_dummyNA <- train_without_dummyNA %>% filter(is.na(year_built) | year_built >=1300) # deletes 4 obs.
```

## Further Adjustments

```{r}

# var format transformations
train_imputed_pmm$price <- sub(pattern = "CHF", replacement = "", train_imputed_pmm$price)
train_imputed_pmm$price <- as.numeric(train_imputed_pmm$price) # price is a factor variable otherwise

# do not use zip code but first two digits of zip code -- otherwise too many clusters having too few observations

train_without_dummyNA$zipcode_11 <- ifelse(train_without_dummyNA$zipcode >= 1100 & train_without_dummyNA$zipcode <= 1199, 1, 0)
train_without_dummyNA$zipcode_12 <- ifelse(train_without_dummyNA$zipcode >= 1200 & train_without_dummyNA$zipcode <= 1299, 1, 0)
train_without_dummyNA$zipcode_13 <- ifelse(train_without_dummyNA$zipcode >= 1300 & train_without_dummyNA$zipcode <= 1399, 1, 0)
train_without_dummyNA$zipcode_14 <- ifelse(train_without_dummyNA$zipcode >= 1400 & train_without_dummyNA$zipcode <= 1499, 1, 0)
train_without_dummyNA$zipcode_15 <- ifelse(train_without_dummyNA$zipcode >= 1500 & train_without_dummyNA$zipcode <= 1599, 1, 0)
train_without_dummyNA$zipcode_16 <- ifelse(train_without_dummyNA$zipcode >= 1600 & train_without_dummyNA$zipcode <= 1699, 1, 0)
train_without_dummyNA$zipcode_17 <- ifelse(train_without_dummyNA$zipcode >= 1700 & train_without_dummyNA$zipcode <= 1799, 1, 0)
train_without_dummyNA$zipcode_18 <- ifelse(train_without_dummyNA$zipcode >= 1800 & train_without_dummyNA$zipcode <= 1899, 1, 0)
train_without_dummyNA$zipcode_19 <- ifelse(train_without_dummyNA$zipcode >= 1900 & train_without_dummyNA$zipcode <= 1999, 1, 0)
train_without_dummyNA$zipcode_20 <- ifelse(train_without_dummyNA$zipcode >= 2000 & train_without_dummyNA$zipcode <= 2099, 1, 0)
train_without_dummyNA$zipcode_21 <- ifelse(train_without_dummyNA$zipcode >= 2100 & train_without_dummyNA$zipcode <= 2199, 1, 0)
train_without_dummyNA$zipcode_22 <- ifelse(train_without_dummyNA$zipcode >= 2200 & train_without_dummyNA$zipcode <= 2299, 1, 0)
train_without_dummyNA$zipcode_23 <- ifelse(train_without_dummyNA$zipcode >= 2300 & train_without_dummyNA$zipcode <= 2399, 1, 0)
train_without_dummyNA$zipcode_24 <- ifelse(train_without_dummyNA$zipcode >= 2400 & train_without_dummyNA$zipcode <= 2499, 1, 0)
train_without_dummyNA$zipcode_25 <- ifelse(train_without_dummyNA$zipcode >= 2500 & train_without_dummyNA$zipcode <= 2599, 1, 0)
train_without_dummyNA$zipcode_26 <- ifelse(train_without_dummyNA$zipcode >= 2600 & train_without_dummyNA$zipcode <= 2699, 1, 0)
train_without_dummyNA$zipcode_27 <- ifelse(train_without_dummyNA$zipcode >= 2700 & train_without_dummyNA$zipcode <= 2799, 1, 0)
train_without_dummyNA$zipcode_28 <- ifelse(train_without_dummyNA$zipcode >= 2800 & train_without_dummyNA$zipcode <= 2899, 1, 0)
train_without_dummyNA$zipcode_29 <- ifelse(train_without_dummyNA$zipcode >= 2900 & train_without_dummyNA$zipcode <= 2999, 1, 0)
train_without_dummyNA$zipcode_30 <- ifelse(train_without_dummyNA$zipcode >= 3000 & train_without_dummyNA$zipcode <= 3099, 1, 0)
train_without_dummyNA$zipcode_31 <- ifelse(train_without_dummyNA$zipcode >= 3100 & train_without_dummyNA$zipcode <= 3199, 1, 0)
train_without_dummyNA$zipcode_32 <- ifelse(train_without_dummyNA$zipcode >= 3200 & train_without_dummyNA$zipcode <= 3299, 1, 0)
train_without_dummyNA$zipcode_33 <- ifelse(train_without_dummyNA$zipcode >= 3300 & train_without_dummyNA$zipcode <= 3399, 1, 0)
train_without_dummyNA$zipcode_34 <- ifelse(train_without_dummyNA$zipcode >= 3400 & train_without_dummyNA$zipcode <= 3499, 1, 0)
train_without_dummyNA$zipcode_35 <- ifelse(train_without_dummyNA$zipcode >= 3500 & train_without_dummyNA$zipcode <= 3599, 1, 0)
train_without_dummyNA$zipcode_36 <- ifelse(train_without_dummyNA$zipcode >= 3600 & train_without_dummyNA$zipcode <= 3699, 1, 0)
train_without_dummyNA$zipcode_37 <- ifelse(train_without_dummyNA$zipcode >= 3700 & train_without_dummyNA$zipcode <= 3799, 1, 0)
train_without_dummyNA$zipcode_38 <- ifelse(train_without_dummyNA$zipcode >= 3800 & train_without_dummyNA$zipcode <= 3899, 1, 0)
train_without_dummyNA$zipcode_39 <- ifelse(train_without_dummyNA$zipcode >= 3900 & train_without_dummyNA$zipcode <= 3999, 1, 0)
train_without_dummyNA$zipcode_40 <- ifelse(train_without_dummyNA$zipcode >= 4000 & train_without_dummyNA$zipcode <= 4099, 1, 0)
train_without_dummyNA$zipcode_41 <- ifelse(train_without_dummyNA$zipcode >= 4100 & train_without_dummyNA$zipcode <= 4199, 1, 0)
train_without_dummyNA$zipcode_42 <- ifelse(train_without_dummyNA$zipcode >= 4200 & train_without_dummyNA$zipcode <= 4299, 1, 0)
train_without_dummyNA$zipcode_43 <- ifelse(train_without_dummyNA$zipcode >= 4300 & train_without_dummyNA$zipcode <= 4399, 1, 0)
train_without_dummyNA$zipcode_44 <- ifelse(train_without_dummyNA$zipcode >= 4400 & train_without_dummyNA$zipcode <= 4499, 1, 0)
train_without_dummyNA$zipcode_45 <- ifelse(train_without_dummyNA$zipcode >= 4500 & train_without_dummyNA$zipcode <= 4599, 1, 0)
train_without_dummyNA$zipcode_46 <- ifelse(train_without_dummyNA$zipcode >= 4600 & train_without_dummyNA$zipcode <= 4699, 1, 0)
train_without_dummyNA$zipcode_47 <- ifelse(train_without_dummyNA$zipcode >= 4700 & train_without_dummyNA$zipcode <= 4799, 1, 0)
train_without_dummyNA$zipcode_48 <- ifelse(train_without_dummyNA$zipcode >= 4800 & train_without_dummyNA$zipcode <= 4899, 1, 0)
train_without_dummyNA$zipcode_49 <- ifelse(train_without_dummyNA$zipcode >= 4900 & train_without_dummyNA$zipcode <= 4999, 1, 0)
train_without_dummyNA$zipcode_50 <- ifelse(train_without_dummyNA$zipcode >= 5000 & train_without_dummyNA$zipcode <= 5099, 1, 0)
train_without_dummyNA$zipcode_51 <- ifelse(train_without_dummyNA$zipcode >= 5100 & train_without_dummyNA$zipcode <= 5199, 1, 0)
train_without_dummyNA$zipcode_52 <- ifelse(train_without_dummyNA$zipcode >= 5200 & train_without_dummyNA$zipcode <= 5299, 1, 0)
train_without_dummyNA$zipcode_53 <- ifelse(train_without_dummyNA$zipcode >= 5300 & train_without_dummyNA$zipcode <= 5399, 1, 0)
train_without_dummyNA$zipcode_54 <- ifelse(train_without_dummyNA$zipcode >= 5400 & train_without_dummyNA$zipcode <= 5499, 1, 0)
train_without_dummyNA$zipcode_55 <- ifelse(train_without_dummyNA$zipcode >= 5500 & train_without_dummyNA$zipcode <= 5599, 1, 0)
train_without_dummyNA$zipcode_56 <- ifelse(train_without_dummyNA$zipcode >= 5600 & train_without_dummyNA$zipcode <= 5699, 1, 0)
train_without_dummyNA$zipcode_57 <- ifelse(train_without_dummyNA$zipcode >= 5700 & train_without_dummyNA$zipcode <= 5799, 1, 0)
train_without_dummyNA$zipcode_58 <- ifelse(train_without_dummyNA$zipcode >= 5800 & train_without_dummyNA$zipcode <= 5899, 1, 0)
train_without_dummyNA$zipcode_59 <- ifelse(train_without_dummyNA$zipcode >= 5900 & train_without_dummyNA$zipcode <= 5999, 1, 0)
train_without_dummyNA$zipcode_60 <- ifelse(train_without_dummyNA$zipcode >= 6000 & train_without_dummyNA$zipcode <= 6099, 1, 0)
train_without_dummyNA$zipcode_61 <- ifelse(train_without_dummyNA$zipcode >= 6100 & train_without_dummyNA$zipcode <= 6199, 1, 0)
train_without_dummyNA$zipcode_62 <- ifelse(train_without_dummyNA$zipcode >= 6200 & train_without_dummyNA$zipcode <= 6299, 1, 0)
train_without_dummyNA$zipcode_63 <- ifelse(train_without_dummyNA$zipcode >= 6300 & train_without_dummyNA$zipcode <= 6399, 1, 0)
train_without_dummyNA$zipcode_64 <- ifelse(train_without_dummyNA$zipcode >= 6400 & train_without_dummyNA$zipcode <= 6499, 1, 0)
train_without_dummyNA$zipcode_65 <- ifelse(train_without_dummyNA$zipcode >= 6500 & train_without_dummyNA$zipcode <= 6599, 1, 0)
train_without_dummyNA$zipcode_66 <- ifelse(train_without_dummyNA$zipcode >= 6600 & train_without_dummyNA$zipcode <= 6699, 1, 0)
train_without_dummyNA$zipcode_67 <- ifelse(train_without_dummyNA$zipcode >= 6700 & train_without_dummyNA$zipcode <= 6799, 1, 0)
train_without_dummyNA$zipcode_68 <- ifelse(train_without_dummyNA$zipcode >= 6800 & train_without_dummyNA$zipcode <= 6899, 1, 0)
train_without_dummyNA$zipcode_69 <- ifelse(train_without_dummyNA$zipcode >= 6900 & train_without_dummyNA$zipcode <= 6999, 1, 0)
train_without_dummyNA$zipcode_70 <- ifelse(train_without_dummyNA$zipcode >= 7000 & train_without_dummyNA$zipcode <= 7099, 1, 0)
train_without_dummyNA$zipcode_71 <- ifelse(train_without_dummyNA$zipcode >= 7100 & train_without_dummyNA$zipcode <= 7199, 1, 0)
train_without_dummyNA$zipcode_72 <- ifelse(train_without_dummyNA$zipcode >= 7200 & train_without_dummyNA$zipcode <= 7299, 1, 0)
train_without_dummyNA$zipcode_73 <- ifelse(train_without_dummyNA$zipcode >= 7300 & train_without_dummyNA$zipcode <= 7399, 1, 0)
train_without_dummyNA$zipcode_74 <- ifelse(train_without_dummyNA$zipcode >= 7400 & train_without_dummyNA$zipcode <= 7499, 1, 0)
train_without_dummyNA$zipcode_75 <- ifelse(train_without_dummyNA$zipcode >= 7500 & train_without_dummyNA$zipcode <= 7599, 1, 0)
train_without_dummyNA$zipcode_76 <- ifelse(train_without_dummyNA$zipcode >= 7600 & train_without_dummyNA$zipcode <= 7699, 1, 0)
train_without_dummyNA$zipcode_77 <- ifelse(train_without_dummyNA$zipcode >= 7700 & train_without_dummyNA$zipcode <= 7799, 1, 0)
train_without_dummyNA$zipcode_78 <- ifelse(train_without_dummyNA$zipcode >= 7800 & train_without_dummyNA$zipcode <= 7899, 1, 0)
train_without_dummyNA$zipcode_79 <- ifelse(train_without_dummyNA$zipcode >= 7900 & train_without_dummyNA$zipcode <= 7999, 1, 0)
train_without_dummyNA$zipcode_80 <- ifelse(train_without_dummyNA$zipcode >= 8000 & train_without_dummyNA$zipcode <= 8099, 1, 0)
train_without_dummyNA$zipcode_81 <- ifelse(train_without_dummyNA$zipcode >= 8100 & train_without_dummyNA$zipcode <= 8199, 1, 0)
train_without_dummyNA$zipcode_82 <- ifelse(train_without_dummyNA$zipcode >= 8200 & train_without_dummyNA$zipcode <= 8299, 1, 0)
train_without_dummyNA$zipcode_83 <- ifelse(train_without_dummyNA$zipcode >= 8300 & train_without_dummyNA$zipcode <= 8399, 1, 0)
train_without_dummyNA$zipcode_84 <- ifelse(train_without_dummyNA$zipcode >= 8400 & train_without_dummyNA$zipcode <= 8499, 1, 0)
train_without_dummyNA$zipcode_85 <- ifelse(train_without_dummyNA$zipcode >= 8500 & train_without_dummyNA$zipcode <= 8599, 1, 0)
train_without_dummyNA$zipcode_86 <- ifelse(train_without_dummyNA$zipcode >= 8600 & train_without_dummyNA$zipcode <= 8699, 1, 0)
train_without_dummyNA$zipcode_87 <- ifelse(train_without_dummyNA$zipcode >= 8700 & train_without_dummyNA$zipcode <= 8799, 1, 0)
train_without_dummyNA$zipcode_88 <- ifelse(train_without_dummyNA$zipcode >= 8800 & train_without_dummyNA$zipcode <= 8899, 1, 0)
train_without_dummyNA$zipcode_89 <- ifelse(train_without_dummyNA$zipcode >= 8900 & train_without_dummyNA$zipcode <= 8999, 1, 0)
train_without_dummyNA$zipcode_90 <- ifelse(train_without_dummyNA$zipcode >= 9000 & train_without_dummyNA$zipcode <= 9099, 1, 0)
train_without_dummyNA$zipcode_91 <- ifelse(train_without_dummyNA$zipcode >= 9100 & train_without_dummyNA$zipcode <= 9199, 1, 0)
train_without_dummyNA$zipcode_92 <- ifelse(train_without_dummyNA$zipcode >= 9200 & train_without_dummyNA$zipcode <= 9299, 1, 0)
train_without_dummyNA$zipcode_93 <- ifelse(train_without_dummyNA$zipcode >= 9300 & train_without_dummyNA$zipcode <= 9399, 1, 0)
train_without_dummyNA$zipcode_94 <- ifelse(train_without_dummyNA$zipcode >= 9400 & train_without_dummyNA$zipcode <= 9499, 1, 0)
train_without_dummyNA$zipcode_95 <- ifelse(train_without_dummyNA$zipcode >= 9500 & train_without_dummyNA$zipcode <= 9599, 1, 0)
train_without_dummyNA$zipcode_96 <- ifelse(train_without_dummyNA$zipcode >= 9600 & train_without_dummyNA$zipcode <= 9699, 1, 0)
train_without_dummyNA$zipcode_97 <- ifelse(train_without_dummyNA$zipcode >= 9700 & train_without_dummyNA$zipcode <= 9799, 1, 0)
train_without_dummyNA$zipcode_98 <- ifelse(train_without_dummyNA$zipcode >= 9800 & train_without_dummyNA$zipcode <= 9899, 1, 0)
train_without_dummyNA$zipcode_99 <- ifelse(train_without_dummyNA$zipcode >= 9900 & train_without_dummyNA$zipcode <= 9999, 1, 0)

# check if there are no observations for zipcode dummy
check_zipcode_dummy <- colSums(train_without_dummyNA == 0)
which(check_zipcode_dummy == dim(train_without_dummyNA)[1]| check_zipcode_dummy == (dim(train_without_dummyNA)[1] -1))
train_without_dummyNA <- subset(train_without_dummyNA, select = -c(zipcode, zipcode_58, zipcode_59, zipcode_78, zipcode_79, zipcode_97, zipcode_98, zipcode_99)) # I also delete the original ZIPCODE variable here. 


# delete some useless or redundant variables
train_without_dummyNA <- subset(train_without_dummyNA, select = -c(id, 
                                                                   address, 
                                                                   area_useable, # too many missing values
                                                                   date, # date of ad publishment should not matter
                                                                   date_available, # too sparse information
                                                                   municipality, # we have zip code
                                                                   sale, # only contains 1's
                                                                   street)) # again, we use zip code

```

Now we cleaned quite some parts from the dataset . We still have area, floors, rooms and year_built with missing data. For these variables, we will use the mice package to impute them.

## Imputation Part

https://stats.stackexchange.com/questions/219013/how-do-the-number-of-imputations-the-maximum-iterations-affect-accuracy-in-mul

--> Informative stackoverflow about imputation methods in mice. 
Although with multiple imputed data sets we would reach better accuracy, it is only advisable to use multiple imputed data sets assuming we have a linear relationship.


```{r}
## multiple core computation


# Counts the number of cores on your machine
K <- parallel::detectCores()

# This registers each core as a member of a
# computing cluster

cl <- makeCluster(K)

registerDoParallel(cl)

# using "pmm" method from MICE package, as it needs quite a lot computation we set m=1 and maxit=1
  train_imputed_pmm <- complete(mice(train_without_dummyNA, m=1, maxit = 20, meth = "pmm", seed = 50))
write.csv(train_imputed_pmm, file = "./train_imputed_pmm.csv", row.names = FALSE)
 
# using "norm.predict" method from MICE package
  train_imputed_norm <- complete(mice(train_without_dummyNA, m=1, maxit = 20, meth = "norm.predict", seed = 50))
write.csv(train_imputed_norm, file = "./train_imputed_norm.csv", row.names = FALSE)
apply(train_imputed_norm,2, pMiss) 

# using "rf" method from MICE package
  train_imputed_rf <- complete(mice(train_without_dummyNA, m=1, maxit = 20, meth = "rf", seed = 50))
write.csv(train_imputed_rf, file = "./train_imputed_rf.csv", row.names = FALSE)
apply(train_imputed_rf,2, pMiss)

# Once you finished, you deactivate the cluster
# with these two commands.
stopCluster(cl)
registerDoSEQ()

```
## Load the imputed dataset

```{r}
train_imputed_pmm <- read.csv("./train_imputed_pmm.csv")
train_imputed_norm <- read.csv("./train_imputed_norm.csv")
train_imputed_rf <- read.csv("./train_imputed_rf.csv")

# Look at the distribution of imputed numeric data, the plot is ugly haha -noo way i Love these plots ++!
# area
plot(density(train_without_dummyNA$area, na.rm = TRUE))
lines(density(train_imputed_pmm$area), col = "red")
lines(density(train_imputed_norm$area), col = "green")
lines(density(train_imputed_rf$area), col = "blue")

# floors
plot(density(train_without_dummyNA$floors, na.rm = TRUE))
lines(density(train_imputed_pmm$floors), col = "red")
lines(density(train_imputed_norm$floors), col = "green")
lines(density(train_imputed_rf$floors), col = "blue")

# rooms
plot(density(train_without_dummyNA$rooms, na.rm = TRUE))
lines(density(train_imputed_pmm$rooms), col = "red")
lines(density(train_imputed_norm$rooms), col = "green")
lines(density(train_imputed_rf$rooms), col = "blue")

# year_built
plot(density(train_without_dummyNA$year_built, na.rm = TRUE))
lines(density(train_imputed_pmm$year_built), col = "red")
lines(density(train_imputed_norm$year_built), col = "green")
lines(density(train_imputed_rf$year_built), col = "blue")

```
## Look at correalation

```{r}

plot(train_imputed_pmm$area, train_imputed_pmm$price) # maybe delete area >600 # relationship seems pretty much linear - but really can't tell, too many observations

plot(train_imputed_pmm$floors, train_imputed_pmm$price) # no relationship detectable

plot(train_imputed_pmm$rooms, train_imputed_pmm$price) # no relationship detectable

plot(train_imputed_pmm$year, train_imputed_pmm$price) # no relationship detectable

plot(train_imputed_pmm$year_built, train_imputed_pmm$price) # no relationship detectable
```

Really hard to detect any relationship, for example to use polynomials in linear regression.


## Linear regression with k fold cross validation.

We include some interaction variables 

```{r}

# Split the data into training and test set

set.seed(123)
training.samples <- train_imputed_pmm$price %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- train_imputed_pmm[training.samples, ]
test.data <- train_imputed_pmm[-training.samples, ]

# Build the model
lm_fit <- lm(price ~., data = train.data)

## test for influential observations via cooks distance
cooksd <- cooks.distance(lm_fit)

plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4*mean(cooksd, na.rm=T), col="green")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="blue")  # add labels
influential <- as.numeric(names(cooksd)[(cooksd > 4*mean(cooksd, na.rm=T))])  # influential row numbers
head(train.data[influential, ])  # influential observations.

# delete influential data points

train.data_without_outliers <- train.data[-influential,] # Don't know why, when I ran this code, it rerurned me an error: Error in xj[i] : only 0's may be mixed with negative subscripts, which spent me the whole morning to deal with...but failed eventually...and I tried this again at noon, it is successfully running


# new model 

lm_fit_without_outliers <- lm(price ~., data = train.data_without_outliers)

# Make predictions and compute the R2, RMSE and MAE
# When running the "predict code", can we ignore this warning message?
# https://stackoverflow.com/questions/26558631/predict-lm-in-a-loop-warning-prediction-from-a-rank-deficient-fit-may-be-mis
# Warning message: In predict.lm(., test.data) :prediction from a rank-deficient fit may be misleading

predictions <- lm_fit %>% predict(test.data)
predictions_without_outliers <- lm_fit_without_outliers %>% predict(test.data)
data.frame( R2 = R2(predictions, test.data$price),
            RMSE = RMSE(predictions, test.data$price),
            MAE = MAE(predictions, test.data$price))

data.frame( R2 = R2(predictions_without_outliers, test.data$price),
            RMSE = RMSE(predictions_without_outliers, test.data$price),
            MAE = MAE(predictions_without_outliers, test.data$price))
            
# Check the residual vs fitted plot, patches of many positive residuals in the middle, but patches of negative residuals at the right-hand side, suggesting linear model maybe not appropriate.
plot(lm_fit)

# Include interaction terms (area:rooms; floors:elvator; area:floors; area:zipcode)
lm_fit1 <- lm(price ~ . + area:rooms + floors:elevator + area:floors 
               + area:zipcode_11 + area:zipcode_12 + area:zipcode_13 + area:zipcode_14 + area:zipcode_15 
               + area:zipcode_16 + area:zipcode_17 + area:zipcode_18 + area:zipcode_19 + area:zipcode_20 
               + area:zipcode_21 + area:zipcode_22 + area:zipcode_23 + area:zipcode_24 + area:zipcode_25 
               + area:zipcode_26 + area:zipcode_27 + area:zipcode_28 + area:zipcode_29 + area:zipcode_30 
               + area:zipcode_31 + area:zipcode_32 + area:zipcode_33 + area:zipcode_34 + area:zipcode_35 
               + area:zipcode_36 + area:zipcode_37 + area:zipcode_38 + area:zipcode_39 + area:zipcode_40  
               + area:zipcode_41 + area:zipcode_42 + area:zipcode_43 + area:zipcode_44 + area:zipcode_45 
               + area:zipcode_46 + area:zipcode_47 + area:zipcode_48 + area:zipcode_49 + area:zipcode_50 
               + area:zipcode_51 + area:zipcode_52 + area:zipcode_53 + area:zipcode_54 + area:zipcode_55 
               + area:zipcode_56 + area:zipcode_57 + area:zipcode_60 + area:zipcode_61 + area:zipcode_62 
               + area:zipcode_63 + area:zipcode_64 + area:zipcode_65 + area:zipcode_66 + area:zipcode_67 
               + area:zipcode_68 + area:zipcode_69 + area:zipcode_70 + area:zipcode_71 + area:zipcode_72 
               + area:zipcode_73 + area:zipcode_74 + area:zipcode_75 + area:zipcode_76 + area:zipcode_77
               + area:zipcode_80 + area:zipcode_81 + area:zipcode_82 + area:zipcode_83 + area:zipcode_84 
               + area:zipcode_85 + area:zipcode_86 + area:zipcode_87 + area:zipcode_88 + area:zipcode_89 
               + area:zipcode_90 + area:zipcode_91 + area:zipcode_92 + area:zipcode_93 + area:zipcode_94 
               + area:zipcode_95 + area:zipcode_96, data = train.data) # If there is a quick way to include these interaction terms?

summary(lm_fit1)
predictions_lm_fit1 <- lm_fit1 %>% predict(test.data)
data.frame( R2 = R2(predictions_lm_fit1, test.data$price),
            RMSE = RMSE(predictions_lm_fit1, test.data$price),
            MAE = MAE(predictions_lm_fit1, test.data$price))
# including the interaction terms lowers RMSE to 702.834.

plot(lm_fit1)
# there is some serious leverage from a couple data points 

# have a try on polynomial regression
lm_fit2 <- lm (price ~ . -area - rooms + area:rooms + floors:elevator + area:floors + poly(area, 4) + poly(rooms, 4)
              + area:zipcode_11 + area:zipcode_12 + area:zipcode_13 + area:zipcode_14 + area:zipcode_15 
              + area:zipcode_16 + area:zipcode_17 + area:zipcode_18 + area:zipcode_19 + area:zipcode_20 
              + area:zipcode_21 + area:zipcode_22 + area:zipcode_23 + area:zipcode_24 + area:zipcode_25 
              + area:zipcode_26 + area:zipcode_27 + area:zipcode_28 + area:zipcode_29 + area:zipcode_30 
              + area:zipcode_31 + area:zipcode_32 + area:zipcode_33 + area:zipcode_34 + area:zipcode_35 
              + area:zipcode_36 + area:zipcode_37 + area:zipcode_38 + area:zipcode_39 + area:zipcode_40  
              + area:zipcode_41 + area:zipcode_42 + area:zipcode_43 + area:zipcode_44 + area:zipcode_45 
              + area:zipcode_46 + area:zipcode_47 + area:zipcode_48 + area:zipcode_49 + area:zipcode_50 
              + area:zipcode_51 + area:zipcode_52 + area:zipcode_53 + area:zipcode_54 + area:zipcode_55 
              + area:zipcode_56 + area:zipcode_57 + area:zipcode_60 + area:zipcode_61 + area:zipcode_62 
              + area:zipcode_63 + area:zipcode_64 + area:zipcode_65 + area:zipcode_66 + area:zipcode_67 
              + area:zipcode_68 + area:zipcode_69 + area:zipcode_70 + area:zipcode_71 + area:zipcode_72 
              + area:zipcode_73 + area:zipcode_74 + area:zipcode_75 + area:zipcode_76 + area:zipcode_77
              + area:zipcode_80 + area:zipcode_81 + area:zipcode_82 + area:zipcode_83 + area:zipcode_84 
              + area:zipcode_85 + area:zipcode_86 + area:zipcode_87 + area:zipcode_88 + area:zipcode_89 
              + area:zipcode_90 + area:zipcode_91 + area:zipcode_92 + area:zipcode_93 + area:zipcode_94 
              + area:zipcode_95 + area:zipcode_96, data = train.data) # I just put the number of 2, 3 and 4 (randomly...)
summary(lm_fit2)
predictions_lm_fit2 <- lm_fit2 %>% predict(test.data)
data.frame( R2 = R2(predictions_lm_fit2, test.data$price),
            RMSE = RMSE(predictions_lm_fit2, test.data$price),
            MAE = MAE(predictions_lm_fit2, test.data$price))
# introduce poly(area, 2) and poly(rooms, 2) RMSE reduces to 699.9686; 
# introduce poly(area, 3) and poly(rooms, 3) RMSE reduces to 696.9234;
# introduce poly(area, 4) and poly(rooms, 4) RMSE reduces to 694.463;
# it seems that intorducing polynomial terms for area and rooms does not obviously reduce RMSE 

plot(lm_fit2)
# model selection
# forward selection
lm_fit1_forward <- regsubsets(price ~ . + area:rooms + floors:elevator + area:floors 
                              + area:zipcode_11 + area:zipcode_12 + area:zipcode_13 + area:zipcode_14 + area:zipcode_15 
                              + area:zipcode_16 + area:zipcode_17 + area:zipcode_18 + area:zipcode_19 + area:zipcode_20 
                              + area:zipcode_21 + area:zipcode_22 + area:zipcode_23 + area:zipcode_24 + area:zipcode_25 
                              + area:zipcode_26 + area:zipcode_27 + area:zipcode_28 + area:zipcode_29 + area:zipcode_30 
                              + area:zipcode_31 + area:zipcode_32 + area:zipcode_33 + area:zipcode_34 + area:zipcode_35 
                              + area:zipcode_36 + area:zipcode_37 + area:zipcode_38 + area:zipcode_39 + area:zipcode_40  
                              + area:zipcode_41 + area:zipcode_42 + area:zipcode_43 + area:zipcode_44 + area:zipcode_45 
                              + area:zipcode_46 + area:zipcode_47 + area:zipcode_48 + area:zipcode_49 + area:zipcode_50 
                              + area:zipcode_51 + area:zipcode_52 + area:zipcode_53 + area:zipcode_54 + area:zipcode_55 
                              + area:zipcode_56 + area:zipcode_57 + area:zipcode_60 + area:zipcode_61 + area:zipcode_62 
                              + area:zipcode_63 + area:zipcode_64 + area:zipcode_65 + area:zipcode_66 + area:zipcode_67 
                              + area:zipcode_68 + area:zipcode_69 + area:zipcode_70 + area:zipcode_71 + area:zipcode_72 
                              + area:zipcode_73 + area:zipcode_74 + area:zipcode_75 + area:zipcode_76 + area:zipcode_77
                              + area:zipcode_80 + area:zipcode_81 + area:zipcode_82 + area:zipcode_83 + area:zipcode_84 
                              + area:zipcode_85 + area:zipcode_86 + area:zipcode_87 + area:zipcode_88 + area:zipcode_89 
                              + area:zipcode_90 + area:zipcode_91 + area:zipcode_92 + area:zipcode_93 + area:zipcode_94 
                              + area:zipcode_95 + area:zipcode_96, data = train.data, nvmax = 118, method = "forward")  # I use lm_fit1 here.

summary <- summary(lm_fit1_forward)
view(summary)
par(mfrow=c(3,2))
plot(summary(lm_fit1_forward)$rss ,xlab="Number of Variables ",ylab="RSS", type="l")
plot(summary(lm_fit1_forward)$rsq ,xlab="Number of Variables ",ylab="Rsq", type="l")
plot(summary(lm_fit1_forward)$adjr2 ,xlab="Number of Variables ",ylab="adjr2", type="l")
plot(summary(lm_fit1_forward)$cp ,xlab="Number of Variables ",ylab="cp", type="l")
plot(summary(lm_fit1_forward)$bic ,xlab="Number of Variables ",ylab="bic", type="l")
# Different criteria gives different numbers of variables that should include in the regression, which confuses me... 
```

Our validation set error is pretty high, and deleting outlier observations does not considerably change this. We should try out different approaches.


## Moving to LASSO
Our data has quite some variables. Especially as we redifined NA's to 0 in the case of dummy variables and imputed the other ones, this can introduce bias. Also, we do not want our model to get into the trap of what is known as "the curse of dimensionality". One way to work against this is to build a LASSO Model - which is designed to force coefficients to get exactly zero. This way, we might be able to drop some variables which do not have good explanational power.

```{r}
# reform X (training and test) and Y training for lasso input
lasso_x <- model.matrix(price ~., train.data)[,-1]
lasso_y <- train.data$price

test_x <- model.matrix(price ~., test.data)[,-1]
test_y <- test.data$price
# fit model 

lasso_fit <- glmnet(lasso_x, lasso_y, alpha = 1)
set.seed(1)
cv.out <- cv.glmnet(lasso_x, lasso_y, alpha = 1)
bestlam <- cv.out$lambda.min
lasso_pred <- predict(lasso_fit, s = bestlam, newx = test_x)
mean((lasso_pred - test_y)^2)

out <- glmnet(lasso_x, lasso_y, alpha = 1)
lasso.coef <- predict(out, type = "coefficients", s= bestlam)[1:20,]
 
lasso.coef[lasso.coef != 0]

## lasso with interaction terms
lasso_y <- as.numeric(lasso_y)
lasso_interact <- hierNet(lasso_x, lasso_y, lam = bestlam)
lasso_interact_testyhat <- predict(lasso_interact, newx = test_x)
mean((test_x - lasso_interact_testyhat)^2)

# no good...

```
Our lasso model has the advantage that there is only 15 predictors left which are used for computing the estimates. Also , our LASSO Model has the best RMSE so far. 

## Walking into the forest for some qualitree times

```{r}
set.seed(1)

tree_fit <- tree(price ~. , data = train.data)
cv_tree <- cv.tree(tree_fit)
tree_testyhat <- predict(tree_fit, newdata = test.data)
mean((test.data$price - tree_testyhat)^2)

```


## Imputation of the test dataset .

```{r}
# substitute NA's in dummy vars with 0
X_test_without_dummyNA <- X_test %>% mutate_at(.vars = dummy_cols, .funs = funs(ifelse(is.na(.), 0, .)))

# delete some useless or redundant variables
X_test_imputed <- subset(X_test_without_dummyNA, select = -c(id, address, area_useable, date, date_available, municipality, street))

apply(X_test_without_dummyNA,2, pMiss)

summary(X_test_without_dummyNA)
## multiple core computation


# Counts the number of cores on your machine
K <- parallel::detectCores()

# This registers each core as a member of a
# computing cluster

cl <- makeCluster(K)

registerDoParallel(cl)

# using "pmm" method from MICE package, as it needs quite a lot computation we set m=1 and maxit=1
  X_test_imputed_pmm <- complete(mice(X_test_imputed, m=2, maxit = 5, meth = "pmm", seed = 50), 1) 
write.csv(X_test_imputed_pmm, file = "./X_test_imputed_pmm.csv", row.names = FALSE)

# using "rf" method from MICE package
  train_imputed_rf <- complete(mice(train_imputed, m=2, maxit = 5, meth = "rf", seed = 50), 1) 
write.csv(train_imputed_rf, file = "./train_imputed_rf.csv", row.names = FALSE)
#apply(train_imputation_rf,2, pMiss)

# Once you finished, you deactivate the cluster
# with these two commands.
stopCluster(cl)
registerDoSEQ()

```
